# ğŸ’¬ Speech to Sign Language Translator

## ğŸ“ Description

This project is a web-based application that translates **spoken words into Indian Sign Language (ISL)** gestures. It uses a combination of **speech recognition**, **NLP**, and **video retrieval** to visually represent spoken language.  
The primary goal is to **bridge the communication gap between hearing and deaf communities** by providing an accessible translation tool.

---

## ğŸš€ Features

- ğŸ™ï¸ **Speech-to-Text Conversion**  
  Captures audio from the user's microphone and converts it into text using speech recognition.

- ğŸ§  **Sign Language Translation**  
  Maps recognized words to pre-recorded ISL video clips.

- ğŸ”¤ **Word-to-Letter Fallback**  
  If a word-level video is unavailable, the system spells it using individual letter videos.

- ğŸ–¥ï¸ **User-Friendly Web Interface**  
  Simple, clean, and easy to use.

---

## ğŸ› ï¸ Technologies Used

### ğŸ”§ Backend:
- **Python** â€“ Core programming language
- **Flask** â€“ Web framework for handling routes and rendering templates
- **speech_recognition** â€“ For converting speech to text
- **moviepy** â€“ For video processing and concatenation
- **Natural Language Processing (NLP)** â€“ Used for tokenizing sentences into individual words
  - Python built-in string methods, or optional libraries like `nltk`
- **Computer Vision (CV)** â€“ Implemented through logic for video retrieval based on processed words

### ğŸ¨ Frontend:
- **HTML/CSS** â€“ For structuring and styling the web interface

---

## ğŸ§  NLP and Computer Vision

### ğŸ—£ï¸ Natural Language Processing (NLP)
The system uses **NLP for tokenization**, breaking down recognized sentences into individual words. This enables the application to search for each wordâ€™s corresponding ISL video.

### ğŸ‘ï¸ Computer Vision (Simplified)
The project applies a simplified form of CV:
- Based on recognized words, it retrieves corresponding video clips from a dataset of pre-recorded ISL gestures.
- If a specific word video is missing, individual letter videos are used to spell the word.

While not using deep learning-based vision models, the logic simulates computer vision behavior through data mapping.

---
## âš™ï¸ Setup and Installation

### 1. Clone the repository:
```bash
git clone [Your-GitHub-Repo-URL]
cd [Your-Project-Folder]
```
### 2. Create a virtual environment (optional but recommended):
```bash
python -m venv venv
# On Unix or MacOS:
source venv/bin/activate
# On Windows:
venv\Scripts\activate
```

### 3. Install dependencies:
```bash
pip install -r requirements.txt
```
```bash
pip freeze > requirements.txt
```

### 4. Run the application:
```bash
python sam2.py
```
---
## ğŸ“ Folder Structure
```
your-project-name/
â”œâ”€â”€ ISL_Gifs/ # Pre-recorded sign language videos for common words
â”œâ”€â”€ letters/ # Videos for each alphabet letter (used when word not found)
â”œâ”€â”€ static/ # Static assets (CSS, images, and output video)
â”‚ â”œâ”€â”€ style.css
â”‚ â””â”€â”€ merged_video.mp4
â”œâ”€â”€ templates/ # HTML templates for frontend
â”‚ â”œâ”€â”€ index.html
â”‚ â””â”€â”€ translate.html
â”œâ”€â”€ sam1.py # Script for text-to-sign translation
â”œâ”€â”€ sam2.py # Main script for speech-to-sign translation
â””â”€â”€ README.md
```
---
## â–¶ï¸ How to Use
1.Open the application in your browser.

2.Click the â€œTranslateâ€ button.

3.Speak into your microphone.

4.The app will:
    .Convert your speech into text
    .Translate the text into sign language video

5.View the generated ISL video on the next screen.

---

## ğŸ“Œ Future Enhancements (Optional)

.Add support for sentence-level ISL grammar

.Integrate animated 3D avatars for dynamic gesture rendering

.Expand the video dataset to cover more vocabulary

.Add multilingual input (e.g., Hindi, Telugu, Tamil)



